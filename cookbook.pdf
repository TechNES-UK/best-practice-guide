%PDF-1.4
%“Œ‹ž ReportLab Generated PDF document http://www.reportlab.com
1 0 obj
<<
/F1 2 0 R /F2 3 0 R /F3 54 0 R /F4 59 0 R
>>
endobj
2 0 obj
<<
/BaseFont /Helvetica /Encoding /WinAnsiEncoding /Name /F1 /Subtype /Type1 /Type /Font
>>
endobj
3 0 obj
<<
/BaseFont /Helvetica-Bold /Encoding /WinAnsiEncoding /Name /F2 /Subtype /Type1 /Type /Font
>>
endobj
4 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 52 0 R /XYZ 62.69291 282.0236 0 ] /Rect [ 62.69291 711.0236 108.2629 723.0236 ] /Subtype /Link /Type /Annot
>>
endobj
5 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 52 0 R /XYZ 62.69291 282.0236 0 ] /Rect [ 527.0227 711.7736 532.5827 723.7736 ] /Subtype /Link /Type /Annot
>>
endobj
6 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 53 0 R /XYZ 62.69291 711.0236 0 ] /Rect [ 62.69291 693.0236 141.0429 705.0236 ] /Subtype /Link /Type /Annot
>>
endobj
7 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 53 0 R /XYZ 62.69291 711.0236 0 ] /Rect [ 527.0227 693.7736 532.5827 705.7736 ] /Subtype /Link /Type /Annot
>>
endobj
8 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 53 0 R /XYZ 62.69291 564.0236 0 ] /Rect [ 62.69291 675.0236 121.0229 687.0236 ] /Subtype /Link /Type /Annot
>>
endobj
9 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 53 0 R /XYZ 62.69291 564.0236 0 ] /Rect [ 527.0227 675.7736 532.5827 687.7736 ] /Subtype /Link /Type /Annot
>>
endobj
10 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 53 0 R /XYZ 62.69291 201.0236 0 ] /Rect [ 62.69291 657.0236 263.3029 669.0236 ] /Subtype /Link /Type /Annot
>>
endobj
11 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 53 0 R /XYZ 62.69291 201.0236 0 ] /Rect [ 527.0227 657.7736 532.5827 669.7736 ] /Subtype /Link /Type /Annot
>>
endobj
12 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 56 0 R /XYZ 62.69291 159.0236 0 ] /Rect [ 82.69291 639.0236 358.3829 651.0236 ] /Subtype /Link /Type /Annot
>>
endobj
13 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 56 0 R /XYZ 62.69291 159.0236 0 ] /Rect [ 527.0227 639.7736 532.5827 651.7736 ] /Subtype /Link /Type /Annot
>>
endobj
14 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 753.0236 0 ] /Rect [ 62.69291 621.0236 158.2629 633.0236 ] /Subtype /Link /Type /Annot
>>
endobj
15 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 753.0236 0 ] /Rect [ 527.0227 621.7736 532.5827 633.7736 ] /Subtype /Link /Type /Annot
>>
endobj
16 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 678.0236 0 ] /Rect [ 82.69291 603.0236 120.4829 615.0236 ] /Subtype /Link /Type /Annot
>>
endobj
17 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 678.0236 0 ] /Rect [ 527.0227 603.7736 532.5827 615.7736 ] /Subtype /Link /Type /Annot
>>
endobj
18 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 420.0236 0 ] /Rect [ 82.69291 585.0236 172.1629 597.0236 ] /Subtype /Link /Type /Annot
>>
endobj
19 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 420.0236 0 ] /Rect [ 527.0227 585.7736 532.5827 597.7736 ] /Subtype /Link /Type /Annot
>>
endobj
20 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 390.0236 0 ] /Rect [ 82.69291 567.0236 169.9329 579.0236 ] /Subtype /Link /Type /Annot
>>
endobj
21 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 390.0236 0 ] /Rect [ 527.0227 567.7736 532.5827 579.7736 ] /Subtype /Link /Type /Annot
>>
endobj
22 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 360.0236 0 ] /Rect [ 62.69291 549.0236 84.36291 561.0236 ] /Subtype /Link /Type /Annot
>>
endobj
23 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 360.0236 0 ] /Rect [ 527.0227 549.7736 532.5827 561.7736 ] /Subtype /Link /Type /Annot
>>
endobj
24 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 327.0236 0 ] /Rect [ 82.69291 531.0236 187.7429 543.0236 ] /Subtype /Link /Type /Annot
>>
endobj
25 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 327.0236 0 ] /Rect [ 527.0227 531.7736 532.5827 543.7736 ] /Subtype /Link /Type /Annot
>>
endobj
26 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 297.0236 0 ] /Rect [ 82.69291 513.0236 150.4929 525.0236 ] /Subtype /Link /Type /Annot
>>
endobj
27 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 297.0236 0 ] /Rect [ 527.0227 513.7736 532.5827 525.7736 ] /Subtype /Link /Type /Annot
>>
endobj
28 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 267.0236 0 ] /Rect [ 82.69291 495.0236 150.5029 507.0236 ] /Subtype /Link /Type /Annot
>>
endobj
29 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 267.0236 0 ] /Rect [ 527.0227 495.7736 532.5827 507.7736 ] /Subtype /Link /Type /Annot
>>
endobj
30 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 237.0236 0 ] /Rect [ 82.69291 477.0236 292.2429 489.0236 ] /Subtype /Link /Type /Annot
>>
endobj
31 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 237.0236 0 ] /Rect [ 527.0227 477.7736 532.5827 489.7736 ] /Subtype /Link /Type /Annot
>>
endobj
32 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 177.0236 0 ] /Rect [ 82.69291 459.0236 187.7229 471.0236 ] /Subtype /Link /Type /Annot
>>
endobj
33 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 57 0 R /XYZ 62.69291 177.0236 0 ] /Rect [ 527.0227 459.7736 532.5827 471.7736 ] /Subtype /Link /Type /Annot
>>
endobj
34 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 58 0 R /XYZ 62.69291 561.0236 0 ] /Rect [ 82.69291 441.0236 188.8529 453.0236 ] /Subtype /Link /Type /Annot
>>
endobj
35 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 58 0 R /XYZ 62.69291 561.0236 0 ] /Rect [ 527.0227 441.7736 532.5827 453.7736 ] /Subtype /Link /Type /Annot
>>
endobj
36 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 60 0 R /XYZ 62.69291 663.0236 0 ] /Rect [ 62.69291 423.0236 261.0629 435.0236 ] /Subtype /Link /Type /Annot
>>
endobj
37 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 60 0 R /XYZ 62.69291 663.0236 0 ] /Rect [ 527.0227 423.7736 532.5827 435.7736 ] /Subtype /Link /Type /Annot
>>
endobj
38 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 60 0 R /XYZ 62.69291 630.0236 0 ] /Rect [ 82.69291 405.0236 126.0329 417.0236 ] /Subtype /Link /Type /Annot
>>
endobj
39 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 60 0 R /XYZ 62.69291 630.0236 0 ] /Rect [ 527.0227 405.7736 532.5827 417.7736 ] /Subtype /Link /Type /Annot
>>
endobj
40 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 64 0 R /XYZ 62.69291 645.0236 0 ] /Rect [ 82.69291 387.0236 202.1629 399.0236 ] /Subtype /Link /Type /Annot
>>
endobj
41 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 64 0 R /XYZ 62.69291 645.0236 0 ] /Rect [ 527.0227 387.7736 532.5827 399.7736 ] /Subtype /Link /Type /Annot
>>
endobj
42 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 64 0 R /XYZ 62.69291 615.0236 0 ] /Rect [ 82.69291 369.0236 156.5929 381.0236 ] /Subtype /Link /Type /Annot
>>
endobj
43 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 64 0 R /XYZ 62.69291 615.0236 0 ] /Rect [ 527.0227 369.7736 532.5827 381.7736 ] /Subtype /Link /Type /Annot
>>
endobj
44 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 753.0236 0 ] /Rect [ 82.69291 351.0236 163.8229 363.0236 ] /Subtype /Link /Type /Annot
>>
endobj
45 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 753.0236 0 ] /Rect [ 521.4627 351.7736 532.5827 363.7736 ] /Subtype /Link /Type /Annot
>>
endobj
46 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 723.0236 0 ] /Rect [ 62.69291 333.0236 210.5029 345.0236 ] /Subtype /Link /Type /Annot
>>
endobj
47 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 723.0236 0 ] /Rect [ 521.4627 333.7736 532.5827 345.7736 ] /Subtype /Link /Type /Annot
>>
endobj
48 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 690.0236 0 ] /Rect [ 62.69291 315.0236 113.8229 327.0236 ] /Subtype /Link /Type /Annot
>>
endobj
49 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 690.0236 0 ] /Rect [ 521.4627 315.7736 532.5827 327.7736 ] /Subtype /Link /Type /Annot
>>
endobj
50 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 657.0236 0 ] /Rect [ 62.69291 297.0236 108.2529 309.0236 ] /Subtype /Link /Type /Annot
>>
endobj
51 0 obj
<<
/Border [ 0 0 0 ] /Contents () /Dest [ 67 0 R /XYZ 62.69291 657.0236 0 ] /Rect [ 521.4627 297.7736 532.5827 309.7736 ] /Subtype /Link /Type /Annot
>>
endobj
52 0 obj
<<
/Annots [ 4 0 R 5 0 R 6 0 R 7 0 R 8 0 R 9 0 R 10 0 R 11 0 R 12 0 R 13 0 R 
  14 0 R 15 0 R 16 0 R 17 0 R 18 0 R 19 0 R 20 0 R 21 0 R 22 0 R 23 0 R 
  24 0 R 25 0 R 26 0 R 27 0 R 28 0 R 29 0 R 30 0 R 31 0 R 32 0 R 33 0 R 
  34 0 R 35 0 R 36 0 R 37 0 R 38 0 R 39 0 R 40 0 R 41 0 R 42 0 R 43 0 R 
  44 0 R 45 0 R 46 0 R 47 0 R 48 0 R 49 0 R 50 0 R 51 0 R ] /Contents 100 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 
  /Trans <<

>> /Type /Page
>>
endobj
53 0 obj
<<
/Contents 101 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
54 0 obj
<<
/BaseFont /Helvetica-Oblique /Encoding /WinAnsiEncoding /Name /F3 /Subtype /Type1 /Type /Font
>>
endobj
55 0 obj
<<
/Contents 102 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
56 0 obj
<<
/Contents 103 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
57 0 obj
<<
/Contents 104 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
58 0 obj
<<
/Contents 105 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
59 0 obj
<<
/BaseFont /Helvetica-BoldOblique /Encoding /WinAnsiEncoding /Name /F4 /Subtype /Type1 /Type /Font
>>
endobj
60 0 obj
<<
/Contents 106 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
61 0 obj
<<
/A <<
/S /URI /Type /Action /URI (https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
>> /Border [ 0 0 0 ] /Rect [ 130.2327 342.0236 531.5661 354.0236 ] /Subtype /Link /Type /Annot
>>
endobj
62 0 obj
<<
/A <<
/S /URI /Type /Action /URI (https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
>> /Border [ 0 0 0 ] /Rect [ 62.69291 330.0236 158.8329 342.0236 ] /Subtype /Link /Type /Annot
>>
endobj
63 0 obj
<<
/Annots [ 61 0 R 62 0 R ] /Contents 107 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 
  /Trans <<

>> /Type /Page
>>
endobj
64 0 obj
<<
/Contents 108 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
65 0 obj
<<
/Contents 109 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
66 0 obj
<<
/Contents 110 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
67 0 obj
<<
/Contents 111 0 R /MediaBox [ 0 0 595.2756 841.8898 ] /Parent 99 0 R /Resources <<
/Font 1 0 R /ProcSet [ /PDF /Text /ImageB /ImageC /ImageI ]
>> /Rotate 0 /Trans <<

>> 
  /Type /Page
>>
endobj
68 0 obj
<<
/Outlines 70 0 R /PageLabels 112 0 R /PageMode /UseNone /Pages 99 0 R /Type /Catalog
>>
endobj
69 0 obj
<<
/Author () /CreationDate (D:20230123171643+00'00') /Creator (\(unspecified\)) /Keywords () /ModDate (D:20230123171643+00'00') /Producer (ReportLab PDF Library - www.reportlab.com) 
  /Subject (\(unspecified\)) /Title () /Trapped /False
>>
endobj
70 0 obj
<<
/Count 34 /First 71 0 R /Last 98 0 R /Type /Outlines
>>
endobj
71 0 obj
<<
/Dest [ 52 0 R /XYZ 62.69291 282.0236 0 ] /Next 72 0 R /Parent 70 0 R /Title (Summary)
>>
endobj
72 0 obj
<<
/Dest [ 53 0 R /XYZ 62.69291 711.0236 0 ] /Next 73 0 R /Parent 70 0 R /Prev 71 0 R /Title (Target Audience)
>>
endobj
73 0 obj
<<
/Dest [ 53 0 R /XYZ 62.69291 564.0236 0 ] /Next 74 0 R /Parent 70 0 R /Prev 72 0 R /Title (Introduction)
>>
endobj
74 0 obj
<<
/Count 1 /Dest [ 53 0 R /XYZ 62.69291 201.0236 0 ] /First 75 0 R /Last 75 0 R /Next 76 0 R /Parent 70 0 R 
  /Prev 73 0 R /Title (Overview: Key Concepts and Terminology)
>>
endobj
75 0 obj
<<
/Dest [ 56 0 R /XYZ 62.69291 159.0236 0 ] /Parent 74 0 R /Title (Putting it Together: Creating Modern AI and Machine Learning)
>>
endobj
76 0 obj
<<
/Count 3 /Dest [ 57 0 R /XYZ 62.69291 753.0236 0 ] /First 77 0 R /Last 79 0 R /Next 80 0 R /Parent 70 0 R 
  /Prev 74 0 R /Title (Running AI Projects)
>>
endobj
77 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 678.0236 0 ] /Next 78 0 R /Parent 76 0 R /Title (Using AI)
>>
endobj
78 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 420.0236 0 ] /Next 79 0 R /Parent 76 0 R /Prev 77 0 R /Title (AI Project Decisions)
>>
endobj
79 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 390.0236 0 ] /Parent 76 0 R /Prev 78 0 R /Title (AI Project Workflow)
>>
endobj
80 0 obj
<<
/Count 6 /Dest [ 57 0 R /XYZ 62.69291 360.0236 0 ] /First 81 0 R /Last 86 0 R /Next 87 0 R /Parent 70 0 R 
  /Prev 76 0 R /Title (Data)
>>
endobj
81 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 327.0236 0 ] /Next 82 0 R /Parent 80 0 R /Title (The Importance of Data)
>>
endobj
82 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 297.0236 0 ] /Next 83 0 R /Parent 80 0 R /Prev 81 0 R /Title (Collecting Data)
>>
endobj
83 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 267.0236 0 ] /Next 84 0 R /Parent 80 0 R /Prev 82 0 R /Title (Managing Data)
>>
endobj
84 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 237.0236 0 ] /Next 85 0 R /Parent 80 0 R /Prev 83 0 R /Title (Validation, Cross Validation and Data Leakage.)
>>
endobj
85 0 obj
<<
/Dest [ 57 0 R /XYZ 62.69291 177.0236 0 ] /Next 86 0 R /Parent 80 0 R /Prev 84 0 R /Title (K-Fold Cross Validation)
>>
endobj
86 0 obj
<<
/Dest [ 58 0 R /XYZ 62.69291 561.0236 0 ] /Parent 80 0 R /Prev 85 0 R /Title (Train, Validate and Test)
>>
endobj
87 0 obj
<<
/Count 8 /Dest [ 60 0 R /XYZ 62.69291 663.0236 0 ] /First 88 0 R /Last 95 0 R /Next 96 0 R /Parent 70 0 R 
  /Prev 80 0 R /Title (Building AI and Machine Learning Models)
>>
endobj
88 0 obj
<<
/Count 2 /Dest [ 60 0 R /XYZ 62.69291 630.0236 0 ] /First 89 0 R /Last 90 0 R /Next 91 0 R /Parent 87 0 R 
  /Title (Decisions)
>>
endobj
89 0 obj
<<
/Dest [ 60 0 R /XYZ 62.69291 600.0236 0 ] /Next 90 0 R /Parent 88 0 R /Title (Data Pipeline)
>>
endobj
90 0 obj
<<
/Dest [ 63 0 R /XYZ 62.69291 477.0236 0 ] /Parent 88 0 R /Prev 89 0 R /Title (Training Process)
>>
endobj
91 0 obj
<<
/Dest [ 64 0 R /XYZ 62.69291 645.0236 0 ] /Next 92 0 R /Parent 87 0 R /Prev 88 0 R /Title (Decision Making Flowchart)
>>
endobj
92 0 obj
<<
/Count 2 /Dest [ 64 0 R /XYZ 62.69291 615.0236 0 ] /First 93 0 R /Last 94 0 R /Next 95 0 R /Parent 87 0 R 
  /Prev 91 0 R /Title (Neural Networks)
>>
endobj
93 0 obj
<<
/Dest [ 64 0 R /XYZ 62.69291 585.0236 0 ] /Next 94 0 R /Parent 92 0 R /Title (Architectural Principles of Neural Networks)
>>
endobj
94 0 obj
<<
/Dest [ 65 0 R /XYZ 62.69291 244.8661 0 ] /Parent 92 0 R /Prev 93 0 R /Title (Neural Network Design and Experimentation Process)
>>
endobj
95 0 obj
<<
/Dest [ 67 0 R /XYZ 62.69291 753.0236 0 ] /Parent 87 0 R /Prev 92 0 R /Title (Worked Examples)
>>
endobj
96 0 obj
<<
/Dest [ 67 0 R /XYZ 62.69291 723.0236 0 ] /Next 97 0 R /Parent 70 0 R /Prev 87 0 R /Title (Pitfalls and Common Problems)
>>
endobj
97 0 obj
<<
/Dest [ 67 0 R /XYZ 62.69291 690.0236 0 ] /Next 98 0 R /Parent 70 0 R /Prev 96 0 R /Title (Resources)
>>
endobj
98 0 obj
<<
/Dest [ 67 0 R /XYZ 62.69291 657.0236 0 ] /Parent 70 0 R /Prev 97 0 R /Title (Appendix)
>>
endobj
99 0 obj
<<
/Count 12 /Kids [ 52 0 R 53 0 R 55 0 R 56 0 R 57 0 R 58 0 R 60 0 R 63 0 R 64 0 R 65 0 R 
  66 0 R 67 0 R ] /Type /Pages
>>
endobj
100 0 obj
<<
/Length 8599
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 732.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Table of Contents) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 294.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 0 417 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Summary) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 417 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (1) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 399 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Target Audience) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 399 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (2) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 381 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Introduction) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 381 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (2) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 363 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Overview: Key Concepts and Terminology) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 363 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (2) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 345 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Putting it Together: Creating Modern AI and Machine Learning) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 345 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (4) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 327 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Running AI Projects) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 327 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 309 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Using AI) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 309 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 291 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (AI Project Decisions) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 291 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 273 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (AI Project Workflow) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 273 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 255 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Data) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 255 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 237 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (The Importance of Data) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 237 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 219 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Collecting Data) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 219 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 201 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Managing Data) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 201 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 183 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Validation, Cross Validation and Data Leakage.) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 183 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 165 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (K-Fold Cross Validation) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 165 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (5) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 147 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Train, Validate and Test) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 147 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (6) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 129 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Building AI and Machine Learning Models) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 129 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 66.44 0 Td (7) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 111 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Decisions) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 111 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (7) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 93 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Decision Making Flowchart) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 93 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (9) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 75 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Neural Networks) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 75 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 66.44 0 Td (9) Tj T* -66.44 0 Td ET
Q
Q
q
1 0 0 1 0 57 cm
q
BT 1 0 0 1 20 2 Tm 12 TL /F1 10 Tf 0 0 .501961 rg (Worked Examples) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 57 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 60.88 0 Td (12) Tj T* -60.88 0 Td ET
Q
Q
q
1 0 0 1 0 39 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Pitfalls and Common Problems) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 39 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 60.88 0 Td (12) Tj T* -60.88 0 Td ET
Q
Q
q
1 0 0 1 0 21 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Resources) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 21 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 60.88 0 Td (12) Tj T* -60.88 0 Td ET
Q
Q
q
1 0 0 1 0 3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 .501961 rg (Appendix) Tj T* ET
Q
Q
q
1 0 0 1 397.8898 3 cm
q
0 0 .501961 rg
0 0 .501961 RG
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL 60.88 0 Td (12) Tj T* -60.88 0 Td ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 261.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Summary) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 135.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F1 10 Tf 12 TL 1.003059 Tw (Artificial Intelligence \(AI\) and Machine Learning \(ML\) technologies are key to many modern engineering) Tj T* 0 Tw 1.44061 Tw (projects due to their ability to solve many problems that are difficult or impossible with other methods.) Tj T* 0 Tw 1.173059 Tw (While most engineers will find themselves enjoying a significant overlap between these techniques and) Tj T* 0 Tw .302846 Tw (their existing skill set, they are also liable to find that AI and Machine Learning is its own field with its own) Tj T* 0 Tw .497318 Tw (unique demands and \(often hidden\) pitfalls. While there are many resources available for self teaching, it) Tj T* 0 Tw .162651 Tw (is generally assumed the practitioner is either an absolute beginner to engineering, or already a seasoned) Tj T* 0 Tw 2.142765 Tw (expert in AI and ML. In this document, we provide a practical guide to AI and Machine learning for) Tj T* 0 Tw .571984 Tw (electronic systems engineers who already have a strong base of knowledge in electronic systems but no) Tj T* 0 Tw .083735 Tw (specialised expertise. This guide will be practice focused, with the goal of helping engineers to make good) Tj T* 0 Tw (decisions and avoid problems. The guide will cover, among many others areas:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 129.0236 cm
Q
q
1 0 0 1 62.69291 129.0236 cm
Q
q
1 0 0 1 62.69291 117.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Technical Language and Core Concepts) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 111.0236 cm
Q
q
1 0 0 1 62.69291 99.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Data, Collection and Common Problems) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 93.02362 cm
Q
q
1 0 0 1 62.69291 81.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Deploying AI and Machine Learning, with Worked Examples) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 76.86614 cm
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
101 0 obj
<<
/Length 6895
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 741.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Existing resources, where to find them and how best to use them) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 735.0236 cm
Q
q
1 0 0 1 62.69291 723.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Common Pitfalls, how to avoid and how to solve) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 723.0236 cm
Q
q
1 0 0 1 62.69291 690.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Target Audience) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 576.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 98 Tm /F1 10 Tf 12 TL .635868 Tw (The target audience for this guide is electronic systems engineers with little to no specific expertise in AI) Tj T* 0 Tw .459318 Tw (and Machine Learning techniques. It will be assumed, because of this background, that readers will have) Tj T* 0 Tw 1.321163 Tw (a base level of competence in programming and mathematics, though the guide will err on the side of) Tj T* 0 Tw 1.090651 Tw (caution in respect of assumed knowledge. We may, for example, assume our reader has knowledge of) Tj T* 0 Tw 2.377485 Tw (concepts in basic calculus, but are unlikely to assume that they are able to remember any specific) Tj T* 0 Tw .971412 Tw (formula. We are also assuming that the primary interest in practically deploying these techniques rather) Tj T* 0 Tw 2.91284 Tw (than understanding the theory and context of their development. For readers interested in a more) Tj T* 0 Tw 1.793984 Tw (theoretical treatment, we list several texts in the resources section appropriate to a range of different) Tj T* 0 Tw (levels of background knowledge.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 543.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Introduction) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 417.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F1 10 Tf 12 TL 2.201318 Tw (The terms Artificial Intelligence and Machine Learning both lend themselves to the idea that we are) Tj T* 0 Tw .234488 Tw (creating a program that can, in some respect, think and learn in the same way a person can. While it may) Tj T* 0 Tw 1.95811 Tw (be possible to create systems that are capable of this, the current technologies that are deployed in) Tj T* 0 Tw .49152 Tw (practice are much more restricted in scope. Current, practical Artificial Intelligence and Machine Learning) Tj T* 0 Tw .978651 Tw (technology generally refers to programs that are of a statistical, or statistically adjacent nature, and that) Tj T* 0 Tw .01784 Tw (use data to fit some specified mathematical model. While these techniques are proving extremely valuable) Tj T* 0 Tw 2.497674 Tw (in solving otherwise difficult or intractable problems, the fundamentals of the techniques are neither) Tj T* 0 Tw 1.279431 Tw (exceptionally novel nor complicated. In this guide, we aim to show just how simple it can be to deploy) Tj T* 0 Tw .294269 Tw (these techniques in practice, especially for an audience that already has knowledge of electronic systems) Tj T* 0 Tw (engineering.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 339.0236 cm
q
BT 1 0 0 1 0 62 Tm 1.088651 Tw 12 TL /F1 10 Tf 0 0 0 rg (It is important to acknowledge that, despite the above, no specific definition of Artificial Intelligence and) Tj T* 0 Tw .715814 Tw (Machine Learning that is universally agreed on exists. Furthermore, lacking any significant advancement) Tj T* 0 Tw -0.090015 Tw (in our understanding of what cognition, intelligence, and learning actually mean, this state of affairs is likely) Tj T* 0 Tw .009488 Tw (to continue. Given the practical focus of this guide, it would not be profitable for us to enter into this debate) Tj T* 0 Tw .374692 Tw (at any length. We will therefore, universally use the term \223AI and Machine Learning\224 to refer to algorithms) Tj T* 0 Tw (that are the subject of this guide, and that learn to ) Tj /F2 10 Tf (parameterize) Tj /F1 10 Tf ( a mathematical model from data.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 213.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 110 Tm /F1 10 Tf 12 TL .569986 Tw (This guide is structured into 5 sections. In the first section, we discuss the key concepts and terminology) Tj T* 0 Tw .907209 Tw (that are used in AI and Machine Learning and briefly discuss how they come together to motivate us to) Tj T* 0 Tw .014692 Tw (create the field in its current form. With this important background covered, we then move on to discussing) Tj T* 0 Tw .275868 Tw (the practicalities of AI and Machine Learning projects at a high level. We discuss where it\222s appropriate to) Tj T* 0 Tw .45284 Tw (deploy these technologies, a high level workflow and decision making process, and what management of) Tj T* 0 Tw .012765 Tw (these projects looks like. Having covered this, we then begin to drill down into the details of these projects.) Tj T* 0 Tw .567488 Tw (We open this discussion with a review of the key role data, and how it can be managed. This is followed) Tj T* 0 Tw .49284 Tw (with an extensive discussion on the implementation and decision making process of deployment of these) Tj T* 0 Tw .60811 Tw (algorithms with worked examples. We close with a list of pitfalls, common problems and solutions, and a) Tj T* 0 Tw (link to useful further resources.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 180.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Overview: Key Concepts and Terminology) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 114.0236 cm
q
BT 1 0 0 1 0 50 Tm .177045 Tw 12 TL /F1 10 Tf 0 0 0 rg (In this section, we introduce and overview some of the key concepts and terminology that are important in) Tj T* 0 Tw .769269 Tw (understanding and discussing AI and machine learning technology. This section is not intended to serve) Tj T* 0 Tw 2.430574 Tw (as an exhaustive discussion of the topics in question, but as an introduction to the key points and) Tj T* 0 Tw 2.597633 Tw (vocabulary that will frame the rest of the discussion on this topic. Key terms and concepts will be) Tj T* 0 Tw /F2 10 Tf (highlighted) Tj /F1 10 Tf ( for the reader, with links to a more detailed description of the term in the appendix.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 84.02362 cm
q
BT 1 0 0 1 0 14 Tm .397485 Tw 12 TL /F1 10 Tf 0 0 0 rg (To briefly review what we discussed in the introduction: the type of intelligence and learning in algorithms) Tj T* 0 Tw .799986 Tw (that we are interested in in this manual are algorithms that learn mathematical models of the world from) Tj T* 0 Tw ET
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
102 0 obj
<<
/Length 7868
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 729.0236 cm
q
BT 1 0 0 1 0 14 Tm 1.02528 Tw 12 TL /F1 10 Tf 0 0 0 rg (some data, in order to make predictions about other unseen or future data. One important idea that we) Tj T* 0 Tw (need to consider first is ) Tj /F2 10 Tf (structured data) Tj /F1 10 Tf ( and ) Tj /F2 10 Tf (unstructured data) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 639.0236 cm
q
BT 1 0 0 1 0 74 Tm .61284 Tw 12 TL /F3 10 Tf 0 0 0 rg (Breakaway: Structured vs Unstructured Data:) Tj /F1 10 Tf ( AI and Machine Learning models are no different from any) Tj T* 0 Tw .904651 Tw (other computer program in that they require their input data to follow a consistent format. Unfortunately,) Tj T* 0 Tw 1.517045 Tw (data collected in the real world rarely follows the type of structure and organization that is needed for) Tj T* 0 Tw .228876 Tw (ingestion by an AI or Machine learning algorithm, and in many cases the work done to transform data into) Tj T* 0 Tw .046457 Tw (an appropriate structured format is some of the most important work done in any AI and Machine Learning) Tj T* 0 Tw 1.923735 Tw (pipeline. We make this distinction between data that has been put into a useful structured format as) Tj T* 0 Tw /F2 10 Tf (structured data) Tj /F1 10 Tf (, and data that exists in a raw, unprocessed format as ) Tj /F2 10 Tf (unstructured data) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 513.0236 cm
q
BT 1 0 0 1 0 110 Tm 1.884724 Tw 12 TL /F1 10 Tf 0 0 0 rg (When dealing with data in the real world, we will often split it up into categories or types. One such) Tj T* 0 Tw .351654 Tw (distinction often made that is especially important in the context of AI and Machine Learning is the split of) Tj T* 0 Tw .42936 Tw (data into ) Tj /F2 10 Tf (continuous) Tj /F1 10 Tf ( data and ) Tj /F2 10 Tf (discrete) Tj /F1 10 Tf ( data. Continuous data can take on any number of infinite values) Tj T* 0 Tw .450514 Tw (across a given range, for example, a measure of rainfall per hour. Discrete data on the other hand is any) Tj T* 0 Tw 1.307765 Tw (type of data that falls into a fixed number of categories. These categories can be both ) Tj /F2 10 Tf (ordinal) Tj /F1 10 Tf ( data in) Tj T* 0 Tw 1.571984 Tw (which there is a natural ordering between the categories \(shoe size, for example\), and ) Tj /F2 10 Tf (nominal) Tj /F1 10 Tf ( data,) Tj T* 0 Tw .052485 Tw (where the categories are distinct \(eye color, for example\). While this distinction is important for many parts) Tj T* 0 Tw 1.347984 Tw (of AI and Machine Learning, the distinction between whether an AI and Machine Learning algorithm is) Tj T* 0 Tw 3.036235 Tw (trying to predict continuous and discrete data is so important that it has its own nomenclature of) Tj T* 0 Tw /F2 10 Tf (regression) Tj /F1 10 Tf ( and ) Tj /F2 10 Tf (classification) Tj /F1 10 Tf ( algorithms respectively.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 447.0236 cm
q
BT 1 0 0 1 0 50 Tm .694976 Tw 12 TL /F3 10 Tf 0 0 0 rg (Breakaway: Regression vs Classification Algorithms: * The distinction between **regression*) Tj /F1 10 Tf ( \(continuous) Tj T* 0 Tw .254269 Tw (output data\) and ) Tj /F2 10 Tf (classification) Tj /F1 10 Tf ( \(discrete output data\) is particularly important in AI and Machine Learning) Tj T* 0 Tw 1.417633 Tw (algorithms, because the type of data that the algorithm outputs has a significant effect on how it must) Tj T* 0 Tw .237318 Tw (function. Notably, some algorithms \(e.g. Support Vector Machines\) are only designed to function in one of) Tj T* 0 Tw (these modalities, and require significant adaptations to perform \(likely very poorly\) in the other.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 345.0236 cm
q
BT 1 0 0 1 0 86 Tm .339398 Tw 12 TL /F1 10 Tf 0 0 0 rg (While we have been discussing some of the concepts and terminology around data to this point, we have) Tj T* 0 Tw -0.12189 Tw (used the terms \223learn\224, \223learning\224 and \223learning from data\224 to describe what our algorithms do without really) Tj T* 0 Tw .509461 Tw (making it explicit what we actually mean by this. One of the reasons that we\222ve avoided doing this is that) Tj T* 0 Tw 1.055318 Tw (\223learning\224 in the context we\222re discussing it is conveniently, without further qualifiers, a term that covers) Tj T* 0 Tw -0.017235 Tw (several different ideas. These differences stem from the way that we use data in order to \223learn\224. The most) Tj T* 0 Tw 3.01152 Tw (prominent of two of these ideas are ) Tj /F2 10 Tf (supervised learning) Tj /F1 10 Tf ( and ) Tj /F2 10 Tf (unsupervised learning) Tj /F1 10 Tf (, which are) Tj T* 0 Tw .36186 Tw (concerned whether we learn from data that list the correct output the algorithms should produce for some) Tj T* 0 Tw (given input data \() Tj /F2 10 Tf (labeled data) Tj /F1 10 Tf (\), or simply the input data themselves \() Tj /F2 10 Tf (unlabeled data) Tj /F1 10 Tf (\).) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 207.0236 cm
q
BT 1 0 0 1 0 122 Tm .48498 Tw 12 TL /F3 10 Tf 0 0 0 rg (Breakaway: Supervised vs Unsupervised vs Reinforcement vs Other Learning:) Tj /F1 10 Tf ( We use the nomenclature) Tj T* 0 Tw .607485 Tw (of ) Tj /F2 10 Tf (Supervised) Tj /F1 10 Tf ( vs ) Tj /F2 10 Tf (Unsupervised) Tj /F1 10 Tf ( \(vs others\) to describe the way in which our algorithms are learning. In) Tj T* 0 Tw .459318 Tw (Supervised learning, we learn from matched input data/output data pairs, data for which we already have) Tj T* 0 Tw .66881 Tw (the correct output the algorithms should predict for a set of given inputs \(\223learning by example\224\). We call) Tj T* 0 Tw 3.682651 Tw (this data ) Tj /F2 10 Tf (labeled data) Tj /F1 10 Tf (, because our set of input data is labeled with the corresponding correct) Tj T* 0 Tw 1.647318 Tw (solutions.For example, we might be interested in predicting the future prices of the stock market from) Tj T* 0 Tw -0.100731 Tw (economic indications, by looking at how these economic indicators have predicted its historical past prices.) Tj T* 0 Tw -0.023349 Tw (In Unsupervised learning, we only have access to the input data without any corresponding output solution) Tj T* 0 Tw .771412 Tw (attached. We call this data ) Tj /F2 10 Tf (unlabeled data) Tj /F1 10 Tf (, and our unsupervised learning algorithms and are generally) Tj T* 0 Tw .609318 Tw (interested in predicting some quality of this data \(\223pattern learning\224\). For example, we might be detecting) Tj T* 0 Tw (unusual anomalies of electrical usage in the grid.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 93.02362 cm
q
0 0 0 rg
BT 1 0 0 1 0 98 Tm /F1 10 Tf 12 TL .779985 Tw (While it is generally preferable to use supervised learning when we can because learning by example is) Tj T* 0 Tw .873059 Tw (easier, there are many situations in which unsupervised approaches are more appropriate. Even putting) Tj T* 0 Tw .59683 Tw (aside the fact that unlabeled data is easier to collect \(since we don\222t need to label it\), for many problems) Tj T* 0 Tw 3.194269 Tw (supervised approaches are simply not practical. In our electrical grid example above, it would be) Tj T* 0 Tw 1.017318 Tw (infeasible to train a supervised model to do similar anomaly detection. By definition, anomalies are rare) Tj T* 0 Tw .467633 Tw (and unusual data points that fall outside of the usual observations in the data. Creating a labeled dataset) Tj T* 0 Tw 1.322339 Tw (of them would be both impractical, and any supervised algorithm that used it would be prescriptive - it) Tj T* 0 Tw 2.114597 Tw (would only catch anomalies similar to anomalies we\222ve trained on, where an unsupervised approach) Tj T* 0 Tw (instead catches ones that are dissimilar to everything we\222ve seen so far.) Tj T* ET
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
103 0 obj
<<
/Length 7021
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 693.0236 cm
q
BT 1 0 0 1 0 50 Tm 1.224147 Tw 12 TL /F1 10 Tf 0 0 0 rg (There are also several other learning approaches that fit within the supervised/unsupervised dichotomy) Tj T* 0 Tw .019983 Tw (discussed so far. A common one is ) Tj /F2 10 Tf (Reinforcement Learning) Tj /F1 10 Tf (. In Reinforcement Learning, the algorithm is) Tj T* 0 Tw .032598 Tw (not fed a set of data, but selects which piece of data it wants to learn from in future from the pieces of data) Tj T* 0 Tw .757984 Tw (it has had up until now. Another common paradigm is ) Tj /F2 10 Tf (semi-supervised learning) Tj /F1 10 Tf (, in which an algorithm) Tj T* 0 Tw (learns from some set data that is labeled, and some \(usually larger\) set of data that is unlabelled.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 579.0236 cm
q
BT 1 0 0 1 0 98 Tm .39104 Tw 12 TL /F1 10 Tf 0 0 0 rg (No matter which of these learning types we want to use, we need to be able to evaluate the performance) Tj T* 0 Tw .508409 Tw (of the AI and Machine Learning models we create. The way we approach this is no different to any other) Tj T* 0 Tw .012765 Tw (testing we would do - we compare the predictions that our model makes to some known ground truth data.) Tj T* 0 Tw .008626 Tw (An easy way to do this would be, once we have ) Tj /F2 10 Tf (trained) Tj /F1 10 Tf ( our algorithm on the data that we have to hand, to) Tj T* 0 Tw .30881 Tw (test how well it performs on this same data \(evaluate the ) Tj /F2 10 Tf (training error) Tj /F1 10 Tf (\) as a ground truth. Unfortunately,) Tj T* 0 Tw .401488 Tw (this is a bad idea. AI and Machine Learning algorithms will fit fairly well to the data they\222ve trained on \(it\222s) Tj T* 0 Tw 2.446235 Tw /F2 10 Tf (training data) Tj /F1 10 Tf (\), independently of how well they work for other \223unseen\224 data. Since the goal of our) Tj T* 0 Tw 1.243145 Tw (algorithm is to have it work well across all data points \(including ones it wasn\222t trained on\), how well it) Tj T* 0 Tw (performs on the training data will be a misleading and overconfident measure of overall performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 465.0236 cm
q
BT 1 0 0 1 0 98 Tm 1.403876 Tw 12 TL /F1 10 Tf 0 0 0 rg (Instead, we try to estimate how well our model will perform on data we\222ve not trained on by randomly) Tj T* 0 Tw 1.279987 Tw (reserving a small amount of our data in a testing set \(our ) Tj /F2 10 Tf (testing data) Tj /F1 10 Tf (\). Sometimes, in addition to the) Tj T* 0 Tw .424198 Tw /F2 10 Tf (training) Tj /F1 10 Tf ( and ) Tj /F2 10 Tf (testing) Tj /F1 10 Tf ( sets we\222ve described, we will make a further split of our data to also include a small) Tj T* 0 Tw .400488 Tw (set of ) Tj /F2 10 Tf (validation) Tj /F1 10 Tf ( data. We might do this if we need to validate the results of testing, for example, in more) Tj T* 0 Tw .638735 Tw (advanced applications in which we might use the testing data itself to make decisions about the learning) Tj T* 0 Tw .104431 Tw (process. It is generally best practice not just to break your data up into training and testing \(and validation,) Tj T* 0 Tw .569398 Tw (if needed\) sets once, but to repeat this process multiple times and aggregate the results. This process is) Tj T* 0 Tw -0.110013 Tw (called cross validation, and in most cases will be the more appropriate way to evaluate our AI and Machine) Tj T* 0 Tw (learning model\222s performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 399.0236 cm
q
BT 1 0 0 1 0 50 Tm .869318 Tw 12 TL /F3 10 Tf 0 0 0 rg (Breakaway: Measures of Performance: * There are multiple ways for us to evaluate performance of any) Tj T* 0 Tw 1.603059 Tw (given model. Some common choices are **Mean Squared Error*) Tj /F1 10 Tf ( \(continuous data\) or ) Tj /F2 10 Tf (Cross Entropy) Tj /F1 10 Tf  T* 0 Tw .187485 Tw (\(discrete data\). It\222s best to stick to standard measures unless you understand what you\222re doing, but there) Tj T* 0 Tw -0.100017 Tw (are usually multiple valid ways of measuring performance with their own consequences. The best measure) Tj T* 0 Tw (of performance is the one that solves your problem best.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 237.0236 cm
q
BT 1 0 0 1 0 146 Tm 1.354104 Tw 12 TL /F1 10 Tf 0 0 0 rg (With a measure of performance, we have a way of comparing different models to select the best one.) Tj T* 0 Tw 1.61186 Tw (Practically though, there are too many different algorithms and approaches for us to run them all and) Tj T* 0 Tw .78186 Tw (directly compare them in this way. We need a way of selecting likely candidates a-priori, without directly) Tj T* 0 Tw -0.044012 Tw (testing them. Our goal in AI and Machine Learning is to make predictions about all of our data from a small) Tj T* 0 Tw 2.00104 Tw (subset of it. We want a model that accurately reflects the reality of the data we\222re training it on. An) Tj T* 0 Tw .847209 Tw (abstraction that can help us think about this is to think in terms of ) Tj /F2 10 Tf (model complexity) Tj /F1 10 Tf (. Our models exist) Tj T* 0 Tw .189988 Tw (broadly on a spectrum of complexity from simple linear models with only a few parameters that fit a line to) Tj T* 0 Tw .536098 Tw (our data at one end, and billion-parameter neural networks at the other. It\222s probably clear to see that an) Tj T* 0 Tw .261988 Tw (overly simple model of our data will be bad. If we can\222t capture the complexity of what is happening in our) Tj T* 0 Tw -0.041512 Tw (data, we\222ll never be able to model it well. We call this ) Tj /F2 10 Tf (underfitting) Tj /F1 10 Tf (. However, it\222s also the case that fitting a) Tj T* 0 Tw 2.031235 Tw (model that is too complex is problematic. Models that are too complicated will fit randomness in the) Tj T* 0 Tw .479988 Tw (specific data they are trained on, and will not generalise well to data outside of that. We call this problem) Tj T* 0 Tw /F2 10 Tf (overfitting) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 171.0236 cm
q
BT 1 0 0 1 0 50 Tm 1.729513 Tw 12 TL /F1 10 Tf 0 0 0 rg (Our goal should be to pick a model that is complicated enough to fit to the parts of the data we are) Tj T* 0 Tw .44332 Tw (interested in \(the ) Tj /F2 10 Tf (signal) Tj /F1 10 Tf (\), without overfitting to the noise in our data too. We also want to take advantage) Tj T* 0 Tw .393672 Tw (of prior knowledge we have about our problem, for example, if we know our problem is linear, it would be) Tj T* 0 Tw 1.187209 Tw (sensible to pick a linear model. When in doubt, it\222s often more favourable to go for simpler models, for) Tj T* 0 Tw (reasons we will discuss later.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 141.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Putting it Together: Creating Modern AI and Machine Learning) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 123.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (WIP) Tj T* ET
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
104 0 obj
<<
/Length 6790
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 732.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Running AI Projects) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 690.0236 cm
q
BT 1 0 0 1 0 26 Tm .777485 Tw 12 TL /F1 10 Tf 0 0 0 rg (In this section, we discuss the problem of designing and managing an AI and Machine Learning project.) Tj T* 0 Tw .290542 Tw (Importantly, this is ) Tj /F3 10 Tf (not) Tj /F1 10 Tf ( a technical guide to solving these problems, but a guide to solving all the problems) Tj T* 0 Tw (that precede and surround the technical parts of the problem.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 660.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Using AI) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 606.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.182093 Tw (The first, and most important problem to solve in any AI and Machine Learning project is to be able to) Tj T* 0 Tw 1.153145 Tw (formulate a clear and concise answer to the question \223why do I want to solve this problem with AI and) Tj T* 0 Tw 1.318314 Tw (Machine Learning\224? AI and Machine Learning algorithms are far from universally appropriate solutions,) Tj T* 0 Tw (and suffer from several fundamental difficulties that make them undesirable:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 600.0236 cm
Q
q
1 0 0 1 62.69291 600.0236 cm
Q
q
1 0 0 1 62.69291 588.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (They require collection and processing of data to feed them) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 582.0236 cm
Q
q
1 0 0 1 62.69291 570.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (They are stochastic, dealing fundamentally in probabilities) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 564.0236 cm
Q
q
1 0 0 1 62.69291 552.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (They are difficult to validate, and further yet, many algorithms are difficult even to interpret) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 552.0236 cm
Q
q
1 0 0 1 62.69291 486.0236 cm
q
BT 1 0 0 1 0 50 Tm .083735 Tw 12 TL /F1 10 Tf 0 0 0 rg (The reason these algorithms have received so much attention ) Tj /F3 10 Tf (despite) Tj /F1 10 Tf ( these difficulties is that they make it) Tj T* 0 Tw -0.117073 Tw (plausible \(or possible\) to solve sets of problems that are otherwise difficult to get at. These challenges, and) Tj T* 0 Tw 1.247765 Tw (the motivation we gave for AI in our previous sections \(understand a large data set by learning from a) Tj T* 0 Tw .189461 Tw (\(relatively\) small amount of data\) speak to a litmus test for whether a problem is suitable to be solved with) Tj T* 0 Tw (AI and Machine learning. A problem is a good candidate if:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 480.0236 cm
Q
q
1 0 0 1 62.69291 480.0236 cm
Q
q
1 0 0 1 62.69291 468.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (It is infeasible to solve the problem in a more direct or analytical way) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 462.0236 cm
Q
q
1 0 0 1 62.69291 450.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (It is feasible to access a useful set of data points to indirectly learn a solution) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 444.0236 cm
Q
q
1 0 0 1 62.69291 432.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (It is infeasible to access all \(or almost all\) of the data points we\222re interested in) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 432.0236 cm
Q
q
1 0 0 1 62.69291 402.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (AI Project Decisions) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 372.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (AI Project Workflow) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 339.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Data) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 309.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (The Importance of Data) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 279.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Collecting Data) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 249.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Managing Data) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 219.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Validation, Cross Validation and Data Leakage.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 189.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .010488 Tw (At some point we are going to need to assess our models\222 performance on unseen data, as this is the only) Tj T* 0 Tw (way to get an unbiased estimate of its performance. There are two popular ways this is done.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 159.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (K-Fold Cross Validation) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 147.0236 cm
Q
q
1 0 0 1 62.69291 147.0236 cm
Q
q
1 0 0 1 62.69291 135.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (1.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 0 rg (Randomly) Tj /F1 10 Tf ( split the dataset into K ) Tj /F2 10 Tf (equal) Tj /F1 10 Tf ( partitions.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 129.0236 cm
Q
q
1 0 0 1 62.69291 117.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (2.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Randomly initialise the model and train on all-but-one of the partitions.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 111.0236 cm
Q
q
1 0 0 1 62.69291 99.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (3.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Test model performance on the withheld partition.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 93.02362 cm
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
105 0 obj
<<
/Length 8863
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 729.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (4.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .064692 Tw (Repeat steps 2 and 3, withholding a different partition each time, until all K partitions have been used) Tj T* 0 Tw (as the test data.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 723.0236 cm
Q
q
1 0 0 1 62.69291 711.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (5.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Combine the K accuracy scores at the end, providing a mean and variancefor the accuracy.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 705.0236 cm
Q
q
1 0 0 1 62.69291 693.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (6.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Make changes to model and repeat until performance is sufficient.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 693.0236 cm
Q
q
1 0 0 1 62.69291 627.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .343988 Tw (This way, all of the available data is used for both training and testing, while never testing a model on the) Tj T* 0 Tw .449988 Tw (data on which it was trained. It also serves to avoid any unintended bias that may arise by chance, when) Tj T* 0 Tw 1.388221 Tw (the data is randomly partitioned. However, it requires training the model from scratch K times to get a) Tj T* 0 Tw 1.26998 Tw (single output. Also, repeatedly running K-Fold Cross Validation and making adjustments, can introduce) Tj T* 0 Tw (implicit bias.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 573.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.062927 Tw (This is well suited to training smaller models on small datasets, as the requirement to train many times) Tj T* 0 Tw .538988 Tw (rules out its use in \221big data\222 applications such as deep learning. If data is limited and your model can be) Tj T* 0 Tw .535697 Tw (trained many times \(at an acceptable cost for you\), K-Fold Cross Validation is recommended. Otherwise,) Tj T* 0 Tw (read on to the next section.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 543.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Train, Validate and Test) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 531.0236 cm
Q
q
1 0 0 1 62.69291 531.0236 cm
Q
q
1 0 0 1 62.69291 495.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (1.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm .751751 Tw 12 TL /F2 10 Tf 0 0 0 rg (Randomly) Tj /F1 10 Tf ( split the dataset into 3 partitions. The proportions of these are up to you, but a sensible) Tj T* 0 Tw 7.92872 Tw (split usually looks something like 80%-10%-10% or 60%-20%-20%, \(train-validate-test,) Tj T* 0 Tw (respectively\)*.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 489.0236 cm
Q
q
1 0 0 1 62.69291 477.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (2.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Repeat until performance is sufficient:) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 477.0236 cm
Q
q
1 0 0 1 62.69291 471.0236 cm
Q
q
1 0 0 1 62.69291 417.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 48 cm
Q
q
1 0 0 1 20 48 cm
Q
q
1 0 0 1 20 36 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (a.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Train the model on the train set.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 18 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (b.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Evaluate on the validation set.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.22 0 Td (c.) Tj T* -6.22 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Make any changes to the model.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 417.0236 cm
Q
q
1 0 0 1 62.69291 411.0236 cm
Q
q
1 0 0 1 62.69291 411.0236 cm
Q
q
1 0 0 1 62.69291 399.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (3.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Evaluate your best performing model\(s\) on the test set.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 399.0236 cm
Q
q
1 0 0 1 62.69291 357.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .15186 Tw (This should be the end of the process, with these results being publication/report ready. One should avoid) Tj T* 0 Tw .217356 Tw (going back and making more changes after this, as that is how implicit bias from the testing set can creep) Tj T* 0 Tw (into the model design \(data leakage\).) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 291.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL 1.18104 Tw (*The larger the datasets, the less variance we can expect between them. So if we expect to do lots of) Tj T* 0 Tw 3.669983 Tw (hyperparameter tuning iterations \(step 2\), we might opt for a 60%-20%-20% split. That way the) Tj T* 0 Tw .310651 Tw (information we gain from the validation step should be more dependable. If not, the 80%-10%-10% would) Tj T* 0 Tw 1.587765 Tw (be preferable simply as we are using more of the data for training, which should give the best model) Tj T* 0 Tw (performance.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 273.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Points to remember:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 267.0236 cm
Q
q
1 0 0 1 62.69291 267.0236 cm
Q
q
1 0 0 1 62.69291 135.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 105 cm
q
BT 1 0 0 1 0 14 Tm 1.866412 Tw 12 TL /F1 10 Tf 0 0 0 rg (We optimise for performance on the training data, but we ) Tj /F2 10 Tf (assess) Tj /F1 10 Tf ( performance on \221unseen\222 \(not) Tj T* 0 Tw (training\) data.) Tj T* ET
Q
Q
q
1 0 0 1 23 99 cm
Q
q
1 0 0 1 23 99 cm
Q
q
1 0 0 1 23 63 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .20061 Tw (If it can perform well on this, we know the model isn\222t simply \221remembering\222 the correct answers) Tj T* 0 Tw 1.653828 Tw (from its training process but has learned meaningful relationships which apply outside of its) Tj T* 0 Tw (training dataset.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 57 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 45 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 50 Tm /F1 10 Tf 12 TL .328876 Tw (We can think of this the same way we might think of assessing the learning ability of a student.) Tj T* 0 Tw .104983 Tw (That being, they \221train\222 on practice questions and are assessed on \221test\222 questions which require) Tj T* 0 Tw 2.485697 Tw (the same skills as the training questions, but the students have never seen them before.) Tj T* 0 Tw 1.69284 Tw (Assessing them on questions they have already seen \(training data\) clearly would not be a) Tj T* 0 Tw (sensible way to evaluate their understanding of the topic.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 129.0236 cm
Q
q
1 0 0 1 62.69291 81.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 21 cm
q
BT 1 0 0 1 0 14 Tm 1.297318 Tw 12 TL /F1 10 Tf 0 0 0 rg (Unseen data can still influence the model, via influencing the ) Tj /F2 10 Tf (decisions we make) Tj /F1 10 Tf ( to improve the) Tj T* 0 Tw (model. This is called ) Tj /F2 10 Tf (data leakage) Tj /F1 10 Tf (.) Tj T* ET
Q
Q
q
1 0 0 1 23 15 cm
Q
q
1 0 0 1 23 15 cm
Q
q
1 0 0 1 23 3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (This is a very subtle problem which can easily go undetected.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
106 0 obj
<<
/Length 8564
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 675.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 6 65 Tm  T* ET
q
1 0 0 1 23 39 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 2.247126 Tw (To avoid this, make the train/validate/test as soon as possible, and leave the test dataset) Tj T* 0 Tw .045318 Tw (untouched until it's time to evaluate our model. This includes not visualising or exploring the test) Tj T* 0 Tw (data, to prevent bias in our own thinking.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 33 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .076303 Tw (This can again be likened to a student taking a test - if they know what questions are coming up) Tj T* 0 Tw 1.27311 Tw (and prepare for those specifically, their test score will end up higher than a true reflection of) Tj T* 0 Tw (their ability.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 675.0236 cm
Q
q
1 0 0 1 62.69291 642.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Building AI and Machine Learning Models) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 612.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Decisions) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 585.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Data Pipeline) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 543.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.464431 Tw (Data Pipeline is simply a term used to describe the moving and processing of data from one place to) Tj T* 0 Tw 3.48936 Tw (another. While this is a broad term, the case we are concerned with specifically is loading and) Tj T* 0 Tw (transforming data from storage in order to be used by a neural network.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 513.0236 cm
q
BT 1 0 0 1 0 14 Tm .407045 Tw 12 TL /F2 10 Tf 0 0 0 rg (Train, test, validate:) Tj /F1 10 Tf ( See earlier section for details. Building the data pipeline is where we must take this) Tj T* 0 Tw (into account and avoid the common pitfalls listed in this section.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 483.0236 cm
q
BT 1 0 0 1 0 14 Tm 1.462651 Tw 12 TL /F2 10 Tf 0 0 0 rg (Reading the data:) Tj /F1 10 Tf ( Whichever machine learning API we use, three core functions typically need to be) Tj T* 0 Tw (implemented for a custom pipeline:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 477.0236 cm
Q
q
1 0 0 1 62.69291 387.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 84 cm
Q
q
1 0 0 1 20 84 cm
Q
q
1 0 0 1 20 72 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Initialise.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 66 cm
Q
q
1 0 0 1 20 42 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .621163 Tw (Get dataset length. In order to know when an epoch is complete, the class needs to know how) Tj T* 0 Tw (much data there is in total. This function is as simple as it sounds.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 36 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 2.439431 Tw (Return the ith datum. A function with a one to one mapping of integers i to data. This is) Tj T* 0 Tw .490574 Tw (important so that the data loader can shuffle the dataset by shuffling a list of integers from zero) Tj T* 0 Tw (to dataset length.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 387.0236 cm
Q
q
1 0 0 1 62.69291 345.0236 cm
q
BT 1 0 0 1 0 26 Tm .02229 Tw 12 TL /F2 10 Tf 0 0 0 rg (Transformations) Tj /F1 10 Tf (. Transforming data is an important step in machine learning. Normalising data is usually) Tj T* 0 Tw .43881 Tw (beneficial in this case, but there may also be transformations specific to your task. A decision that should) Tj T* 0 Tw (be made early on in this process is when to do these transformations. The options are as follows:) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 339.0236 cm
Q
q
1 0 0 1 62.69291 339.0236 cm
Q
q
1 0 0 1 62.69291 207.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Add transformations to the data loading step.) Tj T* ET
Q
Q
q
1 0 0 1 23 111 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 102 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Pros:) Tj T* ET
Q
Q
q
1 0 0 1 20 96 cm
Q
q
1 0 0 1 20 96 cm
Q
q
1 0 0 1 20 84 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Simple, readable) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 78 cm
Q
q
1 0 0 1 20 66 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Easy to change) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 60 cm
Q
q
1 0 0 1 20 48 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Minimal code) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 48 cm
Q
q
1 0 0 1 20 30 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Cons:) Tj T* ET
Q
Q
q
1 0 0 1 20 24 cm
Q
q
1 0 0 1 20 24 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.581984 Tw (Each transform must be applied many times \(every time the data is loaded, once * per) Tj T* 0 Tw (epoch per datum\). This can be slow.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 201.0236 cm
Q
q
1 0 0 1 62.69291 81.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 105 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 105 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Apply transforms to the entire dataset and save in storage.) Tj T* ET
Q
Q
q
1 0 0 1 23 99 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 90 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Pros:) Tj T* ET
Q
Q
q
1 0 0 1 20 84 cm
Q
q
1 0 0 1 20 84 cm
Q
q
1 0 0 1 20 72 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Faster) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 66 cm
Q
q
1 0 0 1 20 54 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Only needs to be considered once) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 54 cm
Q
q
1 0 0 1 20 36 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Cons:) Tj T* ET
Q
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 18 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Harder to quickly experiment with different transformations) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Requires either overwriting original data, or doubling storage requirements) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 76.86614 cm
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
107 0 obj
<<
/Length 7430
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 633.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 105 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 93 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 2.645814 Tw (If computational limitations \(sufficient RAM:dataset size ratio\) allow, there is an optimal middle) Tj T* 0 Tw (ground. This is to use the first method but load all the data into RAM before training.) Tj T* ET
Q
Q
q
1 0 0 1 23 87 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 78 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Pros:) Tj T* ET
Q
Q
q
1 0 0 1 20 72 cm
Q
q
1 0 0 1 20 72 cm
Q
q
1 0 0 1 20 60 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (All the pros of the above methods) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 54 cm
Q
q
1 0 0 1 20 30 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.14248 Tw (In addition to avoiding repeated applications of transforms, it also avoids potentially slow) Tj T* 0 Tw (read times from storage, in favour of fast reading from RAM.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 0 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .63061 Tw (Cons: * High RAM requirements, which increase with dataset size. * Even if HPC allows this, it) Tj T* 0 Tw (cannot be tested on local machines with less RAM, without modification.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 633.0236 cm
Q
q
1 0 0 1 62.69291 555.0236 cm
q
BT 1 0 0 1 0 62 Tm 1.19561 Tw 12 TL /F2 10 Tf 0 0 0 rg (Batch Size:) Tj /F1 10 Tf ( The amount of data the model sees before optimising its parameters at each step. Larger) Tj T* 0 Tw .81436 Tw (batch sizes will give a better estimate of the true optimal parameter adjustment the model should make,) Tj T* 0 Tw .071567 Tw (but each step will take longer to compute. There is no universally optimal number for this. It is advisable to) Tj T* 0 Tw .507765 Tw (use powers of 2, to aid with parallelisation and GPU usage in case this is desired later. Consider starting) Tj T* 0 Tw .176457 Tw (with a batch size of 32 or 64 and increasing/decreasing if the training is unstable/convergence is too slow,) Tj T* 0 Tw (respectively.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 489.0236 cm
q
BT 1 0 0 1 0 50 Tm 1.582927 Tw 12 TL /F2 10 Tf 0 0 0 rg (Epochs:) Tj /F1 10 Tf ( This is the number of full passes through the dataset that the model will see before training) Tj T* 0 Tw .156303 Tw (finishes. It can either be a fixed number, or an upper bound, only reached if the model does not converge.) Tj T* 0 Tw .165984 Tw (The latter scheme is preferable for maximising performance, since early stopping \(end training as soon as) Tj T* 0 Tw 2.017984 Tw (convergence is reached\) is known to be a form of regularisation which can improve performance on) Tj T* 0 Tw (unseen data. The former may be used for comparing training stability of multiple models.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 462.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Training Process) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 444.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (There are a few more decisions to make before we get to designing and training custom neural networks.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 330.0236 cm
q
BT 1 0 0 1 0 98 Tm 1.622765 Tw 12 TL /F2 10 Tf 0 0 0 rg (Loss Function:) Tj /F1 10 Tf ( This is the quantity that the optimiser will try to minimise, thus it is critical to choose) Tj T* 0 Tw .25811 Tw (something that, when minimised, makes the model more useful. This function will be applied to the output) Tj T* 0 Tw -0.050013 Tw (of the network and the target. For example, the mean square error loss function calculates the mean of the) Tj T* 0 Tw 1.92436 Tw (squared error between the output and target, thus minimising this quantity forces the model to try to) Tj T* 0 Tw .49784 Tw (improve prediction accuracy. This is particularly useful for regression problems in which every element of) Tj T* 0 Tw .676235 Tw (the output is equally important. However, if we want a model which predicts a probability distribution, we) Tj T* 0 Tw -0.058016 Tw (may choose to use the categorical cross entropy or KL-Divergence for example. A detailed breakdown can) Tj T* 0 Tw 1.016588 Tw (be found here ) Tj 0 0 .501961 rg (https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-lear) Tj T* 0 Tw (ning-neural-networks/) Tj 0 0 0 rg (.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 204.0236 cm
q
BT 1 0 0 1 0 110 Tm 2.758651 Tw 12 TL /F2 10 Tf 0 0 0 rg (Optimiser:) Tj /F1 10 Tf ( At every step in the optimisation process, the algorithm calculates a way to tweak the) Tj T* 0 Tw 1.649985 Tw (parameters which will reduce the loss function. This tweak is actually a gradient vector, which is only) Tj T* 0 Tw .014724 Tw (locally accurate. This means that the model will improve if we make a small tweak to the parameters in the) Tj T* 0 Tw .351098 Tw (direction of the gradient, how big should that adjustment be? Deciding this is the job of the optimiser, and) Tj T* 0 Tw .229987 Tw (can have a significant effect on how well our model learns. Through a number of tricks such as simulating) Tj T* 0 Tw .853555 Tw (momentum, and warping the loss landscape to make traversal easier, the ADAM optimiser combines all) Tj T* 0 Tw 2.611984 Tw (the most successful optimisation tricks available and is by far the most commonly used. While not) Tj T* 0 Tw 2.538443 Tw (guaranteeing the best performance, it can be relied upon to provide consistently good optimisation.) Tj T* 0 Tw .91811 Tw (Experimenting with other optimisers is an option but not as important as the other considerations in this) Tj T* 0 Tw (list.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 90.02362 cm
q
BT 1 0 0 1 0 98 Tm -0.066941 Tw 12 TL /F2 10 Tf 0 0 0 rg (Validation:) Tj /F1 10 Tf ( Assessing the models performance on the validation dataset regularly during and after training) Tj T* 0 Tw .340574 Tw (to give an indication of how well the model is learning, and compare between model candidates. Here we) Tj T* 0 Tw .40528 Tw (need to consider how frequently we want to validate. Start by validating after every epoch. If more insight) Tj T* 0 Tw .847485 Tw (is required into the learning procedure, increase this to validate after every batch \(or n batches\), though) Tj T* 0 Tw 1.404431 Tw (this will slow down training. * A number of useful tools exist for visualising the validation process. It is) Tj T* 0 Tw 2.074692 Tw (critical to do so, to make sure the model is training as expected. ) Tj /F2 10 Tf (Tensorboard) Tj /F1 10 Tf ( is highly robust and) Tj T* 0 Tw 1.055318 Tw (sufficient for this, and weights and Biases \() Tj /F2 10 Tf (WandB) Tj /F1 10 Tf (\) is a professional subscription service that offers an) Tj T* 0 Tw .178555 Tw (alternative with a more intuitive user interface and cloud services for synchronised training across multiple) Tj T* 0 Tw (devices.) Tj T* ET
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
108 0 obj
<<
/Length 8179
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 657.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 86 Tm /F1 10 Tf 12 TL .498651 Tw (Having done all of this, we are ready to experiment with different model architectures. Model architecture) Tj T* 0 Tw .692126 Tw (design involves skill, intuition, knowledge, and a not insignificant amount of guesswork. Even the world\222s) Tj T* 0 Tw 1.909986 Tw (top ML researchers could not tell you the optimal number of extra neurons to add to improve model) Tj T* 0 Tw .164651 Tw (performance without testing it empirically. However, the good news is that these networks are designed to) Tj T* 0 Tw 1.54881 Tw (give the best performance they can no matter their design, and you are likely to get reasonably good) Tj T* 0 Tw 2.242126 Tw (results within a few attempts. To make this daunting process more systematic, the following section) Tj T* 0 Tw .128409 Tw (details the key architectural principles that go into the design of a model, and the section after will then lay) Tj T* 0 Tw (out a practical guide to follow.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 627.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Decision Making Flowchart) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 597.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Neural Networks) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 570.0236 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Architectural Principles of Neural Networks) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 528.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .166654 Tw (Here we break down some of the key ideas that go into building a more sophisticated neural network than) Tj T* 0 Tw 1.557209 Tw (the basic MLP we used in our first example. Understanding these will allow you to see how even the) Tj T* 0 Tw (world\222s most complex and capable neural networks are put together.) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 522.0236 cm
Q
q
1 0 0 1 62.69291 522.0236 cm
Q
q
1 0 0 1 62.69291 438.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Convolution) Tj T* ET
Q
Q
q
1 0 0 1 23 63 cm
Q
q
1 0 0 1 23 63 cm
Q
q
1 0 0 1 23 51 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 2 Tm 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( A filter with learnable parameters.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 45 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm 1.399213 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( Computer vision has used filters for many decades, designing filters which slide along) Tj T* 0 Tw 1.28248 Tw (\(convolve with\) an image to pick out features such as sharpness, contrast, vertical/horizontal) Tj T* 0 Tw 1.002339 Tw (lines etc. The key insight is that if we instead make the parameters in the filter learnable, the) Tj T* 0 Tw (algorithm can itself determine what the optimal filters should look like, from the data alone.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 432.0236 cm
Q
q
1 0 0 1 62.69291 300.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Pooling) Tj T* ET
Q
Q
q
1 0 0 1 23 111 cm
Q
q
1 0 0 1 23 111 cm
Q
q
1 0 0 1 23 75 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm .244597 Tw 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( A dimensionality reduction technique. Similarly to a convolution, a window is selected on) Tj T* 0 Tw .207984 Tw (the input, and the corresponding output from this window is the largest activation value found in) Tj T* 0 Tw (that window. Then the window is shifted, and the process repeats) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 69 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 62 Tm .146412 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( Dimensionality reduction can be desirable for many reasons, but pooling has a number of) Tj T* 0 Tw 4.153314 Tw (specific advantages. Firstly, it has no parameters to learn, meaning it adds negligible) Tj T* 0 Tw .075814 Tw (computational requirements during training. Secondly it has been found to be complementary to) Tj T* 0 Tw 2.25152 Tw (convolutions. This is likely because convolutions can be thought of as filters searching for) Tj T* 0 Tw .26284 Tw (specific features, and the pooling then essentially tells the network if those features are present) Tj T* 0 Tw (in the given window.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 294.0236 cm
Q
q
1 0 0 1 62.69291 162.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 117 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Convolutional Blocks) Tj T* ET
Q
Q
q
1 0 0 1 23 111 cm
Q
q
1 0 0 1 23 111 cm
Q
q
1 0 0 1 23 87 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm 1.098314 Tw 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( An architectural design shortcut - a combination of convolutional layers, pooling layers) Tj T* 0 Tw (and regularisers packaged into a block which is repeated throughout the network.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 81 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 74 Tm .061984 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( Convolutional layers are used to extract local patterns in the input data. By stacking many) Tj T* 0 Tw -0.051349 Tw (convolutional layers on top of each other, higher level relationships are able to be recognised. In) Tj T* 0 Tw .885366 Tw (practice, this means the network is able to perform better \(more complex\) pattern recognition,) Tj T* 0 Tw 1.951984 Tw (which is what machine learning is all about. Using a repeating block structure like the one) Tj T* 0 Tw .839213 Tw (pictured below has been found to be very effective. Each block contains multiple convolutions) Tj T* 0 Tw 2.92284 Tw (and a pooling layer to reduce dimensionality. Each element of the block is optional and) Tj T* 0 Tw (customisable, but the principle of repeating blocks remains the same.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 156.0236 cm
Q
q
1 0 0 1 62.69291 84.02362 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Skip Connections) Tj T* ET
Q
Q
q
1 0 0 1 23 51 cm
Q
q
1 0 0 1 23 51 cm
Q
q
1 0 0 1 23 3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm 1.58686 Tw 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( Passing the output of one layer in the network directly to later layers in the network,) Tj T* 0 Tw 1.01152 Tw (\221skipping\222 over the intermediate layers. This can be done through addition, which requires the) Tj T* 0 Tw 1.65152 Tw (dimensions of the layers to be equal, or through appending, which increases the dimension) Tj T* 0 Tw (size. Appending is the safer choice, though comes at greater computational cost.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
109 0 obj
<<
/Length 8639
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 705.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 6 35 Tm  T* ET
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 38 Tm 1.24061 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( As we make our networks deeper, we are able to extract higher level features. This is) Tj T* 0 Tw .14784 Tw (extremely powerful, but some new issues begin to emerge. Firstly, the low level information can) Tj T* 0 Tw -0.11119 Tw (get lost on the way. Secondly, we can run into the vanishing gradient problem. A neat solution to) Tj T* 0 Tw (diminish both of these issues is to use skip connections.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 699.0236 cm
Q
q
1 0 0 1 62.69291 555.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 129 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 129 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Bottlenecks) Tj T* ET
Q
Q
q
1 0 0 1 23 123 cm
Q
q
1 0 0 1 23 123 cm
Q
q
1 0 0 1 23 87 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 26 Tm 1.13561 Tw 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( A bottleneck refers to the shape of the network \(big to small\). Often followed by more) Tj T* 0 Tw .08881 Tw (layers to build the network size back up \(small to big\). The bottleneck itself is the smallest layer,) Tj T* 0 Tw (which can also be called the encoding layer.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 81 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 74 Tm .413828 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( Many networks utilise the idea of a bottleneck, even beyond simple autoencoders \(which) Tj T* 0 Tw .968443 Tw (are nothing more than a bottleneck in structure\). Compressing data through a small encoding) Tj T* 0 Tw .661412 Tw (layer encourages the network to extract the most distinguishing features from the data. This is) Tj T* 0 Tw .573876 Tw (used in a number of different ways. The encoding itself can be used to represent the data in a) Tj T* 0 Tw 1.209318 Tw (unique and low dimensional form. Or the second half of the network can use the information) Tj T* 0 Tw .874651 Tw (from skip connections and the encoding to infer high level information about the data to solve) Tj T* 0 Tw (problems.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 549.0236 cm
Q
q
1 0 0 1 62.69291 429.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 105 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 105 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Recurrence) Tj T* ET
Q
Q
q
1 0 0 1 23 99 cm
Q
q
1 0 0 1 23 99 cm
Q
q
1 0 0 1 23 75 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 14 Tm -0.010602 Tw 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( A network layer which takes as input some data and a \221state\222 vector, and produces a new) Tj T* 0 Tw (state vector alongside its other output.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 69 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 62 Tm .023318 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( The state vector represents some understanding about the state at a given time. The idea) Tj T* 0 Tw 1.221654 Tw (then is for the layer to take in some data and update this understanding, so that the state is) Tj T* 0 Tw .717485 Tw (different for the next time step. Without recurrence, networks have no notion of time or way to) Tj T* 0 Tw .058735 Tw (relate the data coming in now with what came before. This is necessary for sequential tasks like) Tj T* 0 Tw .859269 Tw (video or text recognition, and not necessary for static tasks such as image recognition, hence) Tj T* 0 Tw (the discrepancy.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 424.8661 cm
Q
q
1 0 0 1 62.69291 256.8661 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 153 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 153 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Attention) Tj T* ET
Q
Q
q
1 0 0 1 23 147 cm
Q
q
1 0 0 1 23 147 cm
Q
q
1 0 0 1 23 75 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 62 Tm .701412 Tw 12 TL /F2 10 Tf 0 0 0 rg (What:) Tj /F1 10 Tf ( Weighting each element of a sequence of data, according to how important \(how much) Tj T* 0 Tw 1.758735 Tw (attention should be paid to\) it, to solve a given task. Typically, the model will be outputting) Tj T* 0 Tw 1.097485 Tw (another sequence, and as such each element of the output with will require a different set of) Tj T* 0 Tw .64561 Tw (attention weights. The full theory behind attention is beyond the scope of this guide. It is listed) Tj T* 0 Tw 1.416412 Tw (here to give a brief intuition behind transformer models, which are growing in popularity and) Tj T* 0 Tw (based on the principle of attention.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 69 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 57 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 10.5 0 Td (\177) Tj T* -10.5 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
BT 1 0 0 1 0 62 Tm 1.481235 Tw 12 TL /F2 10 Tf 0 0 0 rg (Why:) Tj /F1 10 Tf ( Attention provides a way for a network to take in sequential data all at once, learning) Tj T* 0 Tw .966905 Tw (which input elements each output should pay attention to. This offers numerous benefits over) Tj T* 0 Tw 1.483828 Tw (recurrence, such as the ability to be processed in parallel \(recurrent networks are inherently) Tj T* 0 Tw 2.253314 Tw (sequential so cannot be parallelised\), and removing recency bias. Recency bias being the) Tj T* 0 Tw .207984 Tw (tendency of RNNs to pay more attention to the most recent sequence elements, rather than the) Tj T* 0 Tw (most relevant.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 256.8661 cm
Q
q
1 0 0 1 62.69291 229.8661 cm
q
BT 1 0 0 1 0 2.5 Tm 15 TL /F4 12.5 Tf 0 0 0 rg (Neural Network Design and Experimentation Process) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 217.8661 cm
Q
q
1 0 0 1 62.69291 217.8661 cm
Q
q
1 0 0 1 62.69291 133.8661 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 69 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (1.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 74 Tm /F1 10 Tf 12 TL .62561 Tw (Establish a strong baseline. The first thing to do once your data pipeline and evaluation metrics are) Tj T* 0 Tw .589431 Tw (set up, is to try out the simplest neural network design relevant to your problem. Usually this will be) Tj T* 0 Tw .644724 Tw (an MLP. The data will determine the input and output size, so for this make a simple MLP with one) Tj T* 0 Tw .334988 Tw (hidden layer. There is no magic formula to tell you how large to make this hidden layer, the key is to) Tj T* 0 Tw .887485 Tw (experiment. Start with \(input size + output size\)/2 if you are unsure. Train and evaluate. This gives) Tj T* 0 Tw .236303 Tw (you a benchmark and some idea of how complex the task will be. If the MLP performs very well, you) Tj T* 0 Tw (may wish to stop there. If not, move on to step 2.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 62.69291 127.8661 cm
Q
q
1 0 0 1 62.69291 76.86614 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 36 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (2.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 12 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL .878555 Tw (Design a neural network specifically for your problem. How? Google it! More specifically, scour the) Tj T* 0 Tw 1.413828 Tw (internet for a research paper, article or competition submission that publishes a machine learning) Tj T* 0 Tw .219987 Tw (model for a problem similar to yours. All kinds of similarity are useful, working on the same data type) Tj T* 0 Tw ET
Q
Q
q
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
110 0 obj
<<
/Length 8225
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 597.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 6 143 Tm  T* ET
q
1 0 0 1 23 105 cm
q
0 0 0 rg
BT 1 0 0 1 0 38 Tm /F1 10 Tf 12 TL 1.94784 Tw (\(eg time series, video etc\), or the same problem \(eg anomaly detection, object recognition\), but) Tj T* 0 Tw .702093 Tw (ideally both. For this to be successful, it is likely that the authors have already done a great deal of) Tj T* 0 Tw 1.947485 Tw (the work for us in choosing approximately the right kind of network architecture. Take this as a) Tj T* 0 Tw (starting point.) Tj T* ET
Q
Q
q
1 0 0 1 23 99 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 96 cm
Q
q
1 0 0 1 20 96 cm
Q
q
1 0 0 1 20 72 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (a.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 4.117126 Tw (The closer their problem is to yours, the less we need to experiment with other) Tj T* 0 Tw (architectures) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 66 cm
Q
q
1 0 0 1 20 30 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (b.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 26 Tm /F1 10 Tf 12 TL 1.701647 Tw (For research papers in particular, models may be more complicated than necessary as) Tj T* 0 Tw .243828 Tw (authors are usually proposing a novel method. We can experiment with removing the more) Tj T* 0 Tw (complicated features, if they don\222t affect performance.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 24 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 9 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.22 0 Td (c.) Tj T* -6.22 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL 1.653059 Tw (If nothing relevant arises, use the baseline model we established earlier as the starting) Tj T* 0 Tw (point.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 591.0236 cm
Q
q
1 0 0 1 62.69291 249.0236 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 327 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (3.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 243 cm
q
0 0 0 rg
BT 1 0 0 1 0 86 Tm /F1 10 Tf 12 TL .20936 Tw (Iteration and experimentation. Given a baseline \(either from step 1 or 2\) and working data pipeline, it) Tj T* 0 Tw 3.660814 Tw (may be surprisingly straightforward to test different ideas, so long as enough computational) Tj T* 0 Tw .834985 Tw (resources are available. Bear your initial goals in mind, don\222t be afraid to stop iterating when these) Tj T* 0 Tw 3.967318 Tw (are met even if it may be possible to squeeze slightly more performance out with further) Tj T* 0 Tw .884147 Tw (experimentation. Very small improvements to measured performance on test data may not actually) Tj T* 0 Tw 1.633318 Tw (translate to a significantly better model in the real world. APIs such as TensorFlow and PyTorch) Tj T* 0 Tw .630651 Tw (make adjusting model architectures as simple as stacking preset functions on top of one and other.) Tj T* 0 Tw (These all come in built with these APIs, and are listed below:) Tj T* ET
Q
Q
q
1 0 0 1 23 237 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 234 cm
Q
q
1 0 0 1 20 234 cm
Q
q
1 0 0 1 20 168 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 51 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (a.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 39 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .415529 Tw (Regularisers: These may improve generalisation performance without changing the overall) Tj T* 0 Tw (architecture.) Tj T* ET
Q
Q
q
1 0 0 1 23 33 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 18 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 9 0 Td (i.) Tj T* -9 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Dropout) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.78 0 Td (ii.) Tj T* -6.78 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Batch normalisation.) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 20 162 cm
Q
q
1 0 0 1 20 114 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 33 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (b.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 21 cm
q
0 0 0 rg
BT 1 0 0 1 0 14 Tm /F1 10 Tf 12 TL .959147 Tw (Dimensionality Manipulations: Scale up or down dimensionality. Often comes at a cost of) Tj T* 0 Tw (information loss in favour of computational efficiency.) Tj T* ET
Q
Q
q
1 0 0 1 23 15 cm
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 2 Tm  T* ET
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 9 0 Td (i.) Tj T* -9 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Max Pooling) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 20 108 cm
Q
q
1 0 0 1 20 57 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 36 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.22 0 Td (c.) Tj T* -6.22 0 Td ET
Q
Q
q
1 0 0 1 23 36 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL (Recurrent modules: for sequential data only.) Tj T* ET
Q
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 26 Tm  T* ET
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 18 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 9 0 Td (i.) Tj T* -9 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (LSTM) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.78 0 Td (ii.) Tj T* -6.78 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (GRU) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
Q
Q
q
1 0 0 1 20 51 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 36 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 5.66 0 Td (d.) Tj T* -5.66 0 Td ET
Q
Q
q
1 0 0 1 23 36 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F2 10 Tf 12 TL (Other) Tj T* ET
Q
Q
q
1 0 0 1 23 -3 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
BT 1 0 0 1 0 26 Tm  T* ET
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 30 cm
Q
q
1 0 0 1 20 18 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 9 0 Td (i.) Tj T* -9 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Convolutions) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 12 cm
Q
q
1 0 0 1 20 0 cm
0 0 0 rg
BT /F1 10 Tf 12 TL ET
q
1 0 0 1 6 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 6.78 0 Td (ii.) Tj T* -6.78 0 Td ET
Q
Q
q
1 0 0 1 23 -3 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL (Skip Connections) Tj T* ET
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
Q
Q
q
1 0 0 1 20 0 cm
Q
q
Q
Q
q
1 0 0 1 23 -3 cm
Q
q
Q
Q
q
1 0 0 1 62.69291 249.0236 cm
Q
q
1 0 0 1 62.69291 171.0236 cm
q
0 0 0 rg
BT 1 0 0 1 0 62 Tm /F1 10 Tf 12 TL 1.777633 Tw (Taking the ideas of others as inspiration, try out some of these ideas and observe their effects, both) Tj T* 0 Tw .830651 Tw (individually and in combination. See the architectural principles section for a breakdown of what each of) Tj T* 0 Tw .10104 Tw (these do. Remember, simplicity is key. A useful method to avoid wasted time is to train and test the model) Tj T* 0 Tw .601751 Tw (after a given small change, one change at a time. If performance doesn\222t improve, don\222t waste any more) Tj T* 0 Tw .752209 Tw (time on changes of that kind. Whereas if you make many changes and then evaluate, you can\222t be sure) Tj T* 0 Tw (which changes are having a positive effect.) Tj T* ET
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
111 0 obj
<<
/Length 635
>>
stream
1 0 0 1 0 0 cm  BT /F1 12 Tf 14.4 TL ET
q
1 0 0 1 62.69291 735.0236 cm
q
BT 1 0 0 1 0 3 Tm 18 TL /F2 15 Tf 0 0 0 rg (Worked Examples) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 702.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Pitfalls and Common Problems) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 669.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Resources) Tj T* ET
Q
Q
q
1 0 0 1 62.69291 636.0236 cm
q
BT 1 0 0 1 0 3.5 Tm 21 TL /F2 17.5 Tf 0 0 0 rg (Appendix) Tj T* ET
Q
Q
q
1 0 0 1 56.69291 773.1969 cm
q
0 0 0 rg
BT 1 0 0 1 0 2 Tm /F1 10 Tf 12 TL 163.4149 0 Td (TechNES AI Best Practices Group:) Tj T* -163.4149 0 Td ET
Q
Q
 
endstream
endobj
112 0 obj
<<
/Nums [ 0 113 0 R 1 114 0 R 2 115 0 R 3 116 0 R 4 117 0 R 
  5 118 0 R 6 119 0 R 7 120 0 R 8 121 0 R 9 122 0 R 
  10 123 0 R 11 124 0 R ]
>>
endobj
113 0 obj
<<
/S /D /St 1
>>
endobj
114 0 obj
<<
/S /D /St 2
>>
endobj
115 0 obj
<<
/S /D /St 3
>>
endobj
116 0 obj
<<
/S /D /St 4
>>
endobj
117 0 obj
<<
/S /D /St 5
>>
endobj
118 0 obj
<<
/S /D /St 6
>>
endobj
119 0 obj
<<
/S /D /St 7
>>
endobj
120 0 obj
<<
/S /D /St 8
>>
endobj
121 0 obj
<<
/S /D /St 9
>>
endobj
122 0 obj
<<
/S /D /St 10
>>
endobj
123 0 obj
<<
/S /D /St 11
>>
endobj
124 0 obj
<<
/S /D /St 12
>>
endobj
xref
0 125
0000000000 65535 f 
0000000073 00000 n 
0000000136 00000 n 
0000000243 00000 n 
0000000355 00000 n 
0000000523 00000 n 
0000000691 00000 n 
0000000859 00000 n 
0000001027 00000 n 
0000001195 00000 n 
0000001363 00000 n 
0000001532 00000 n 
0000001701 00000 n 
0000001870 00000 n 
0000002039 00000 n 
0000002208 00000 n 
0000002377 00000 n 
0000002546 00000 n 
0000002715 00000 n 
0000002884 00000 n 
0000003053 00000 n 
0000003222 00000 n 
0000003391 00000 n 
0000003560 00000 n 
0000003729 00000 n 
0000003898 00000 n 
0000004067 00000 n 
0000004236 00000 n 
0000004405 00000 n 
0000004574 00000 n 
0000004743 00000 n 
0000004912 00000 n 
0000005081 00000 n 
0000005250 00000 n 
0000005419 00000 n 
0000005588 00000 n 
0000005757 00000 n 
0000005926 00000 n 
0000006095 00000 n 
0000006264 00000 n 
0000006433 00000 n 
0000006602 00000 n 
0000006771 00000 n 
0000006940 00000 n 
0000007109 00000 n 
0000007278 00000 n 
0000007447 00000 n 
0000007616 00000 n 
0000007785 00000 n 
0000007954 00000 n 
0000008123 00000 n 
0000008292 00000 n 
0000008461 00000 n 
0000009022 00000 n 
0000009229 00000 n 
0000009345 00000 n 
0000009552 00000 n 
0000009759 00000 n 
0000009966 00000 n 
0000010173 00000 n 
0000010293 00000 n 
0000010500 00000 n 
0000010761 00000 n 
0000011022 00000 n 
0000011255 00000 n 
0000011462 00000 n 
0000011669 00000 n 
0000011876 00000 n 
0000012083 00000 n 
0000012190 00000 n 
0000012448 00000 n 
0000012523 00000 n 
0000012632 00000 n 
0000012762 00000 n 
0000012889 00000 n 
0000013081 00000 n 
0000013230 00000 n 
0000013403 00000 n 
0000013513 00000 n 
0000013648 00000 n 
0000013769 00000 n 
0000013927 00000 n 
0000014051 00000 n 
0000014181 00000 n 
0000014309 00000 n 
0000014470 00000 n 
0000014608 00000 n 
0000014734 00000 n 
0000014927 00000 n 
0000015077 00000 n 
0000015192 00000 n 
0000015310 00000 n 
0000015450 00000 n 
0000015619 00000 n 
0000015764 00000 n 
0000015915 00000 n 
0000016032 00000 n 
0000016175 00000 n 
0000016299 00000 n 
0000016409 00000 n 
0000016551 00000 n 
0000025203 00000 n 
0000032151 00000 n 
0000040072 00000 n 
0000047146 00000 n 
0000053989 00000 n 
0000062905 00000 n 
0000071522 00000 n 
0000079005 00000 n 
0000087237 00000 n 
0000095929 00000 n 
0000104207 00000 n 
0000104894 00000 n 
0000105055 00000 n 
0000105090 00000 n 
0000105125 00000 n 
0000105160 00000 n 
0000105195 00000 n 
0000105230 00000 n 
0000105265 00000 n 
0000105300 00000 n 
0000105335 00000 n 
0000105370 00000 n 
0000105406 00000 n 
0000105442 00000 n 
trailer
<<
/ID 
[<456af3e4d7c3c61635a9a5ab7a4356e2><456af3e4d7c3c61635a9a5ab7a4356e2>]
% ReportLab generated PDF document -- digest (http://www.reportlab.com)

/Info 69 0 R
/Root 68 0 R
/Size 125
>>
startxref
105478
%%EOF
